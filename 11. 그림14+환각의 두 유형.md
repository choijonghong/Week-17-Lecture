# **LLM 환각(Hallucination) 유형 및 메커니즘 심층 분석**

본 보고서는 대형 언어 모델(LLM)에서 발생하는 환각 현상의 정의, 유형별 특징, 발생 원인 및 대응 전략을 '그림 14'의 사례를 중심으로 심층 분석합니다.

## **1\. 환각(Hallucination)의 정의와 분류체계**

LLM에서의 **환각**은 모델이 생성한 텍스트가 사실과 다르거나, 논리적으로 타당하지 않거나, 주어진 입력(Context)과 일치하지 않는 현상을 의미합니다. 이는 크게 두 가지 축으로 분류됩니다.

| 구분 | 내재적 환각 (Intrinsic) | 외재적 환각 (Extrinsic) |
| :---- | :---- | :---- |
| **핵심 개념** | **신뢰성(Faithfulness) 결여** | **사실성(Factuality) 결여** |
| **판단 기준** | 주어진 컨텍스트(Source)와의 모순 여부 | 외부 세계의 실제 사실(World Knowledge)과의 일치 여부 |
| **데이터 의존성** | 입력 데이터 내에 답이 있음에도 틀림 | 입력 데이터에 정보가 없어 내부 지식에 의존하다 틀림 |

## **2\. 환각 유형별 상세 분석 (그림 14 사례 중심)**

### **(a) 내재적 환각 (Intrinsic Hallucination)**

*"입력된 맥락 안에서 논리적 일관성을 유지하지 못하는 경우"*

* **사례 분석**:  
  * **입력**: "Bob의 아내는 Amy이고, Bob의 딸은 Cindy다."  
  * **모델 답변**: "Cindy는 Amy의 며느리(daughter-in-law)다."  
  * **오류 유형**: **관계 추론 오류(Relational Reasoning Error)**. '딸의 어머니는 아내'라는 자명한 논리 구조를 무시하고 엉뚱한 관계를 설정함.  
* **특징**: 외부 지식 없이 오직 제공된 지문만으로도 '오답'임을 즉각 판별할 수 있습니다. 주로 요약, 번역, 논리적 추론 작업에서 발생합니다.

### **(b) 외재적 환각 (Extrinsic Hallucination)**

*"입력되지 않은 외부 사실에 대해 틀린 정보를 '그럴듯하게' 생성하는 경우"*

* **사례 분석**:  
  * **질문**: "LLM을 위한 RLHF에 대해 설명해줘."  
  * **모델 답변**: "RLHF는 'Rights, Limitations, Harms, and Freedoms'의 약자다."  
  * **오류 유형**: **지식 충돌 및 창작(Knowledge Conflict & Fabrication)**. RLHF의 실제 정의(Reinforcement Learning from Human Feedback)를 무시하고, 약어의 철자에만 맞춘 허구의 정의를 생성함.  
* **특징**: 답변 자체는 문법적으로 완벽하고 논리적으로 보일 수 있어, 사용자가 해당 분야의 전문가가 아닐 경우 속기 쉽습니다.

## **3\. 환각 발생의 기술적 원인 (Root Causes)**

### **3.1. 아키텍처 및 학습 메커니즘의 한계**

1. **확률적 단어 예측 (Next Token Prediction)**: 모델은 '진실'을 찾는 것이 아니라 '다음에 올 확률이 가장 높은 단어'를 선택합니다. 이 과정에서 유창성(Fluency)이 정확성(Accuracy)을 압도할 때 환각이 발생합니다.  
2. **어텐션 메커니즘의 병목 (Attention Bottleneck)**: 입력 문장이 길거나 복잡할 경우, 모델이 문장 사이의 핵심적인 논리적 연결 고리(예: 가족 관계)를 놓치는 '어텐션 실패'가 발생합니다.

### **3.2. 데이터 및 최적화 과정의 문제**

1. **학습 데이터의 노이즈**: 인터넷상의 잘못된 정보나 상충하는 데이터를 학습하여 모델 내부 지식 자체가 오염된 경우입니다.  
2. **데이터 편향 및 빈도 효과**: 학습 데이터에 자주 등장하는 패턴을 무리하게 적용(과잉 일반화)하여, 생소한 약어나 개념을 익숙한 방식으로 잘못 해석합니다.

## **4\. 환각 완화 및 관리 전략**

### **4.1. 기술적 솔루션: RAG와 CoT**

* **검색 증강 생성 (RAG, Retrieval-Augmented Generation)**:  
  * **원리**: 모델의 내부 지식 대신, 검증된 외부 데이터베이스에서 관련 문서를 검색하여 참조하게 함.  
  * **효과**: 특히 **외재적 환각**을 줄이는 데 탁월하며, 답변의 근거(Source)를 제시할 수 있어 신뢰도를 높입니다.  
* **단계별 사고 (CoT, Chain-of-Thought)**:  
  * **원리**: "단계별로 생각해보자"와 같은 프롬프트를 통해 논리적 추론 과정을 노출시킴.  
  * **효과**: **내재적 환각** 완화에 효과적이며, 모델이 중간 단계에서 스스로 오류를 수정할 기회를 제공합니다.

### **4.2. 시스템적 검증 메커니즘**

* **자기 검증 (Self-Correction)**: 모델에게 자신이 생성한 답변의 논리적 모순을 다시 한번 검토하도록 시키는 반복 루프를 설계합니다.  
* **N-문항 교차 검증**: 동일한 질문을 여러 번 수행하거나 서로 다른 모델(Cross-model)을 사용하여 답변의 일관성을 확인합니다.

## **5\. 결론 및 제언**

환각은 LLM의 결함이라기보다는 **확률적 생성 방식에서 기인하는 본질적인 특성**에 가깝습니다. 따라서 이를 완전히 제거하는 것은 현재 기술적으로 불가능에 가깝습니다.

성공적인 LLM 활용을 위해서는 다음의 세 가지 접근이 필수적입니다:

1. **용도에 따른 구분**: 창의적 글쓰기에는 환각이 '상상력'이 되지만, 정보 검색에는 '치명적 오류'가 됨을 인지해야 합니다.  
2. **검증 프로세스의 내재화**: RAG 시스템 구축 및 사실 확인(Fact-check) 파이프라인 도입이 필수입니다.  
3. **사용자 리터러시**: AI의 답변을 비판적으로 수용하고 최종 검증은 인간이 수행하는 'Human-in-the-loop' 구조를 유지해야 합니다.