# **대규모 언어 모델(LLM) 사전학습 데이터 출처 분석 보고서**

본 보고서는 주요 LLM들의 사전학습 데이터 구성을 분석하여, 데이터 출처가 모델의 성능과 특성에 미치는 영향을 고찰합니다.
<img width="1138" height="488" alt="image" src="https://github.com/user-attachments/assets/b9d08f5a-a4dd-4253-ab39-46b17d078533" />

## **1\. 데이터 출처의 분류 및 주요 특성**

학습 데이터의 종류는 모델이 습득하는 지식의 범위와 문제 해결 방식을 결정짓는 핵심 요소입니다.

| 데이터 유형 | 주요 소스 (예시) | 핵심 학습 목표 및 효과 |
| :---- | :---- | :---- |
| **Webpages (웹페이지)** | CommonCrawl, C4 | 광범위한 상식, 최신 트렌드, 다양한 주제에 대한 범용성 확보 |
| **Conversation (대화)** | Reddit, StackExchange | 자연스러운 문맥 파악, 대화 흐름 이해, 챗봇의 상호작용 능력 강화 |
| **Books & News (도서/뉴스)** | BookCorpus, CC-News | 논리적인 문장 구조, 심도 있는 문맥 이해, 고품질 텍스트 생성 |
| **Scientific Data (과학)** | arXiv, PubMed | 전문 용어 학습, 학술적 논리 구조, 전문 분야의 정확도 향상 |
| **Code (코드)** | GitHub, BigQuery | 알고리즘 로직 이해, 프로그래밍 언어 구문, 논리적 추론 능력 강화 |

## **2\. 모델별 데이터 구성 전략 분석**

각 모델은 개발 목적에 따라 데이터의 혼합 비율을 전략적으로 조정합니다.

### **2.1. 범용 및 웹 중심 모델 (General-Purpose)**

* **웹 데이터 집중형 (T5, Falcon):** 데이터의 100%를 웹에서 추출하여 광범위한 정보 수집에 집중합니다.  
* **범용 지식 지향형 (LLaMA, GPT-3):** 웹 데이터(84\~87%)를 기반으로 도서 및 뉴스 데이터를 보완하여 범용성과 논리성을 동시에 추구합니다.

### **2.2. 품질 및 대화 특화 모델 (Quality & Interaction)**

* **고품질 텍스트 지향형 (Gopher, Chinchilla, GLAM):** 단순 웹 크롤링보다 정제된 도서 및 뉴스 데이터 비중을 높여 신뢰도 높은 콘텐츠 생성을 목표로 합니다. (특히 GLAM은 문헌 비중 48%)  
* **대화 및 멀티미디어 지향형 (PaLM, LaMDA):** 대화 데이터 비중을 50%까지 높여 인간과의 자연스러운 상호작용에 최적화된 성능을 보입니다.

### **2.3. 도메인 특화 모델 (Domain-Specific)**

* **과학 연구 특화 (Galactica):** 과학 데이터 비중을 86%까지 극대화하여 전문 학술 도메인에 최적화되었습니다.  
* **프로그래밍 특화 (AlphaCode, CodeGen):** 코드 데이터를 핵심(AlphaCode 100%, CodeGen 39%)으로 활용하여 문제 해결 능력을 극대화했습니다.  
* **다재다능형 (GPT-NeoX):** 5가지 데이터 소스를 가장 고르게 배분하여 균형 잡힌 성능을 목표로 합니다.

## **3\. 주요 시사점 및 미래 전망**

### **1\) 범용성에서 전문성으로의 확장**

초기 모델들이 방대한 웹 데이터를 통해 '무엇이든 할 수 있는' 범용성에 집중했다면, 최신 모델들은 특정 도메인(과학, 코드 등)에 집중하여 '특정 분야를 완벽히 수행하는' 전문성을 확보하는 추세입니다.

### **2\) '데이터 양'보다 '데이터 질'의 시대**

단순히 크롤링된 웹 데이터의 양을 늘리기보다, 검증된 학술지, 도서, 정제된 뉴스의 비중을 높여 모델의 할루시네이션(환각 현상)을 줄이고 답변의 신뢰도를 높이는 전략이 주를 이루고 있습니다.

### **3\) 목적 지향적 아키텍처와 데이터의 결합**

모델의 서비스 목적(챗봇, 코드 어시스턴트, 연구 보조 등)에 맞춘 데이터셋 조합(Data Mix) 최적화가 LLM 경쟁력의 핵심 요소로 자리 잡았습니다. 향후에는 더욱 미세하게 조정된(Fine-tuned) 도메인 특화 데이터셋의 가치가 더욱 상승할 것으로 보입니다.

*본 분석은 제공된 데이터 출처 비율 비교 자료를 바탕으로 작성되었습니다.*
