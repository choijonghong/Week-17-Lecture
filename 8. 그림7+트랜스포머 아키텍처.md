# **트랜스포머 아키텍처의 3가지 유형 비교 분석**

이 문서는 인공지능 모델(Transformer)이 문장을 이해하고 생성하는 세 가지 핵심 아키텍처를 비교합니다. 각 모델의 핵심 차이점은 **'어텐션(Attention) 가시성'**, 즉 모델이 특정 시점에서 어떤 단어들을 참고할 수 있는가에 있습니다.

## **1\. 어텐션 맵(Attention Map) 색상 가이드**

그림에서 표현된 각 색상은 모델의 정보 참조 범위를 나타냅니다.

* 🔵 **파란색 (Fully Visible / Prefix):** 입력 데이터(Prefix) 내에서 모든 단어가 서로를 자유롭게 참조하는 양방향(Bidirectional) 관계입니다.  
* 🟢 **초록색 (Cross-Attention):** 생성되는 단어(Target)가 입력된 문맥(Prefix) 전체를 꼼꼼히 훑어보는 관계입니다.  
* 🟡 **노란색 (Causal / Self-Attention):** 생성 중인 단어가 '자신보다 이전에 생성된' 단어들만 참고하는 단방향(Unidirectional) 관계입니다.  
* ⚫ **회색 (Masked):** 미래의 정보를 미리 알 수 없도록 가려진 부분입니다.
<img width="1329" height="498" alt="image" src="https://github.com/user-attachments/assets/b99980d0-ffab-48a0-9d00-80d727577c32" />

## **2\. 아키텍처별 심층 분석**

### **① Causal Decoder (GPT 계열)**

**"과거를 통해 미래를 쓰는 외눈박이 작가"**

* **작동 방식:** 입력을 별도로 구분하지 않고, 모든 단어를 순차적으로 생성합니다. 다음에 올 단어를 예측할 때 오직 '지나온 단어들'만 볼 수 있습니다.  
* **어텐션 특징:** 🟡(이전 단어 참조)와 ⚫(미래 단어 마스킹)으로만 구성됩니다.  
* **강점:** 창의적인 문장 생성, 소설 집필, 대화 흐름 이어가기.  
* **대표 모델:** GPT-3, GPT-4, Llama.

### **② Prefix Decoder (UniLM, GLM 계열)**

**"주제 파악 후 본론을 쓰는 전략가"**

* **작동 방식:** 입력값(Prefix)에 대해서는 전체를 동시에 파악(양방향)하고, 이후 정답이나 본문(Target)을 생성할 때는 순차적으로 생성합니다.  
* **어텐션 특징:** 🔵(입력부 양방향 이해) \+ 🟢(입력부 참조) \+ 🟡(출력부 순차 참조)를 혼합하여 사용합니다.  
* **강점:** 주어진 문맥에 대한 정확한 이해가 필요한 질의응답(QA), 조건부 텍스트 생성.  
* **대표 모델:** UniLM, GLM, PaLM(일부).

### **③ Encoder-Decoder (T5, BART 계열)**

**"완벽한 이해 후 재창조하는 전문 통역사"**

* **작동 방식:** '읽기 전담팀(Encoder)'이 입력 문장을 끝까지 다 읽고 의미를 응축하면, '쓰기 전담팀(Decoder)'이 그 정보를 바탕으로 새로운 언어나 형태로 재구성합니다.  
* **어텐션 특징:** Encoder는 전체가 🔵(양방향), Decoder는 Encoder를 🟢(참조)하며 🟡(순차 생성)합니다.  
* **강점:** 언어 번역(KOR→ENG), 긴 문서 요약, 문장 교정.  
* **대표 모델:** T5, BART, Original Transformer(Attention is All You Need).

## **3\. 핵심 아키텍처 종합 비교**

| 구분 | Causal Decoder | Prefix Decoder | Encoder-Decoder |
| :---- | :---- | :---- | :---- |
| **비유** | 순차적으로 써 내려가는 작가 | 주제문을 숙지하고 쓰는 작가 | 원문을 다 듣고 번역하는 통역사 |
| **어텐션 방향** | 단방향 (Left-to-Right) | 혼합 (양방향 Prefix \+ 단방향 Target) | 분리 (양방향 Encoder \+ 단방향 Decoder) |
| **가장 잘하는 것** | **텍스트 생성** (창작, 채팅) | **문맥 기반 생성** (질의응답) | **텍스트 변환** (번역, 요약) |
| **데이터 활용** | 입력과 출력을 구분 없이 학습 | 입력부의 관계를 더 밀접하게 학습 | 입력과 출력의 관계(Mapping)를 학습 |
| **대표 모델** | GPT, HyperCLOVA | UniLM, GLM | T5, BART |

## **4\. 요약 및 결론**

어떤 아키텍처를 선택할지는 \*\*"모델이 풀어야 할 문제의 성격"\*\*에 따라 결정됩니다.

1. **자유로운 대화와 창작**이 목표라면 **Causal Decoder**가 가장 효율적입니다.  
2. **주어진 정보를 정확히 분석**하여 답을 내야 한다면 **Prefix Decoder**가 유리합니다.  
3. 한 형태의 정보를 \*\*다른 형태의 정보로 변환(Transduction)\*\*해야 한다면 **Encoder-Decoder**가 가장 강력한 성능을 발휘합니다.
