# **RLHF (Reinforcement Learning with Human Feedback) 워크플로우 상세 분석**

RLHF는 대규모 언어 모델(LLM)을 인간의 의도와 가치관에 맞게 조정하는 \*\*'정렬(Alignment)'\*\*의 핵심 기술입니다. 본 문서는 RLHF의 3단계 과정을 상세히 분석하고, 기술적 보완점과 핵심 원칙을 정리합니다.

## **0\. RLHF의 목적: 정렬(Alignment)**

단순한 사전 학습(Pre-training)만 거친 모델은 인터넷의 방대한 데이터를 학습했기에 부적절하거나 무의미한 답변을 생성할 수 있습니다. RLHF는 모델이 다음 세 가지 기준(**HHH**)을 충족하도록 만듭니다.

* **Helpful (유용성):** 사용자의 지시를 정확히 이해하고 도움이 되는 정보를 제공함.  
* **Honest (진실성):** 허위 정보를 지어내지 않고(Hallucination 방지) 사실에 기반함.  
* **Harmless (무해성):** 편향, 혐오 표현, 위험한 정보를 생성하지 않음.

## **1\. 단계별 워크플로우 (Three-Step Process)**

### **1단계: Supervised Fine-tuning (SFT, 지도 미세조정)**

모델에게 "바람직한 답변이란 이런 것이다"라는 기본 가이드라인을 학습시키는 과정입니다.

* **과정:** 전문 작업자(Annotator)가 엄격한 가이드라인에 따라 질문(Prompt)에 대한 고품질 답변(Demonstration)을 직접 작성합니다.  
* **핵심:** 데이터의 양보다 \*\*질(Quality)\*\*이 중요합니다. 이 과정을 통해 모델은 지시어(Instruction)를 따르는 법을 배웁니다.  
* **결과물:** SFT 모델 (Baseline Model)

### **2단계: Reward Model Training (RM, 보상 모델 학습)**

인간의 '선호도'를 수치화할 수 있는 '채점 모델'을 만드는 과정입니다.

* **과정:**  
  1. SFT 모델이 하나의 질문에 대해 여러 개의 답변 후보(A, B, C, D)를 생성합니다.  
  2. 인간이 이를 보고 순위를 매깁니다 (예: D \> B \> C \> A).  
  3. 이 비교 데이터를 바탕으로, 답변의 품질을 점수(Scalar Reward)로 출력하는 별도의 **보상 모델**을 학습시킵니다.  
* **기술적 포인트:** 두 답변 중 어느 것이 더 나은지 판단하는 'Elo Rating' 방식이나 'Pairwise Comparison' 손실 함수가 사용됩니다.  
* **결과물:** 보상 모델 (Reward Model)

### **3단계: RL Fine-tuning (강화학습 미세조정)**

보상 모델을 심판으로 두고, 모델이 스스로 최적의 답변을 찾아가도록 훈련하는 최종 최적화 단계입니다.

* **과정:**  
  1. 모델이 새로운 질문에 대해 답변을 생성합니다.  
  2. 보상 모델이 해당 답변에 점수를 부여합니다.  
  3. **PPO(Proximal Policy Optimization)** 알고리즘을 사용하여 보상 점수를 극대화하는 방향으로 모델의 파라미터를 업데이트합니다.  
* **중요 메커니즘 (KL Divergence):** 강화학습 과정에서 모델이 보상 점수만 따려고 언어가 파괴되는 현상(Reward Hacking)을 방지하기 위해, 기존 SFT 모델의 분포에서 너무 멀어지지 않도록 제약 조건을 겁니다.  
* **결과물:** 정렬된 언어 모델 (Aligned LLM, 예: ChatGPT)

## **2\. RLHF의 주요 이점 및 한계**

### **이점**

1. **인간 가치 반영:** 정교한 수학적 공식으로 정의하기 어려운 '품질'이나 '윤리'를 모델에 이식할 수 있습니다.  
2. **안전성 강화:** 탈옥(Jailbreaking) 시도나 유해한 요청에 대해 거절하는 능력이 향상됩니다.

### **한계 및 보완점**

1. **비용 문제:** 숙련된 인간 작업자가 데이터를 생성하고 순위를 매기는 데 막대한 비용과 시간이 소요됩니다.  
2. **주관성:** 작업자(Annotator)마다 선호도가 다를 경우 모델에 편향이 생길 수 있습니다.  
3. **RLAIF (AI Feedback):** 최근에는 인간 대신 더 성능이 좋은 AI 모델이 피드백을 주는 방식(RLAIF)으로 효율성을 극대화하기도 합니다.

## **3\. 요약 및 시사점**

| 단계 | 명칭 | 역할 | 비유 |
| :---- | :---- | :---- | :---- |
| **Step 1** | **SFT** | 기본 교육 | 교과서와 모범 답안으로 공부하기 |
| **Step 2** | **RM** | 채점 기준 확립 | 선생님의 채점 방식을 배운 자동 채점기 만들기 |
| **Step 3** | **RL** | 실전 훈련 | 자동 채점기에게 높은 점수를 받도록 스스로 연습하기 |

RLHF는 단순히 기술적인 최적화를 넘어, 인공지능이 인류에게 유익하고 안전한 도구가 되도록 만드는 핵심적인 '윤리적 여과 장치' 역할을 수행합니다.